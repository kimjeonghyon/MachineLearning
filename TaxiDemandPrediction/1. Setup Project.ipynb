{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi Dataset Project - Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Steps\n",
    "\n",
    "**Step 0:** Prerequisites\n",
    "\n",
    "**Step 1:** Setup Spark cluster in AWS (do this from the shell/git bash)\n",
    "\n",
    "**Step 2:** Sanity check to ensure Spark & S3 are setup properly\n",
    "\n",
    "**Step 3:** Upload data files into S3 (you can skip this and use my S3 location)\n",
    "\n",
    "**Step 4:** Check cluster performance on dataset\n",
    "\n",
    "In the end I have also added other **uselful notes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setup Spark cluster in AWS & perform sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Note: Run all the items in Step 1 from unix shell/Git bash (not from the Jupyter notebook)**\n",
    "\n",
    "#### Step 1a) Create the cluster in AWS: run the following command in unix shell/Git bash (you may change the instance type/count)\n",
    "\n",
    "**Enhancements incorporated to the script**: \n",
    "1. Setup Spark to use the maximum available resources (the myConfig.json file has the instructions)\n",
    "2. Download the admin application Ganglia"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "export CLUSTER_ID=`aws emr create-cluster --name \"CS109 Proj Spark cluster\" \\\n",
    "--release-label emr-4.2.0 --applications Name=Spark Name=Ganglia --ec2-attributes KeyName=CS109 \\\n",
    "--instance-type c3.2xlarge --instance-count 5 --configurations file://myConfig.json --use-default-roles \\\n",
    "--bootstrap-actions Path=s3://cs109-2015/install-anaconda-emr,Name=Install_Anaconda \\\n",
    "--query 'ClusterId' --output text` && echo $CLUSTER_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1c) Wait for the cluster to be ready: AWS web console has to show \"WAITING\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "aws emr describe-cluster --cluster-id $CLUSTER_ID --query 'Cluster.Status.State' --output text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1d)  Get the cluster master's IP:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "export DNS_NAME=`aws emr describe-cluster --cluster-id $CLUSTER_ID \\\n",
    "--query 'Cluster.MasterPublicDnsName' --output text` && echo $DNS_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1e) Run the script to configure Spark "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ssh -o ServerAliveInterval=10 -i CS109.pem hadoop@$DNS_NAME 'sh configure-spark.sh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1f) Create an SSH tunel to the AWS box and connect to the cluster. This command assumes your SSH key is on the same directory you are invoking the SSH command from. At the end of this you will be in a terminal session on the cluster's master node."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ssh -o ServerAliveInterval=10 -i CS109.pem hadoop@$DNS_NAME -L 8989:localhost:8888"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1f) Open your browser and got to http://localhost:8989\n",
    "\n",
    "Note: The notebook you open will **already** have the spark context set up for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Sanity check to ensure Spark & S3 are setup properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2a) Upload this Jupyter Notebook using the console from http://localhost:8989\n",
    "\n",
    "Note: all the steps in Step 2 are to be executed from the Jupyter Notebook iteself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2b) Setup the SparkContext (automatically setup by YARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7fe2fa205310>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'local[*]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Just an informational message\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2c) Spark sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "rdd = sc.parallelize(xrange(10),10)\n",
    "aa = rdd.map(lambda x: sys.version)\n",
    "aa.cache()\n",
    "aa.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2d) S3 sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/parallels/Documents/TaxiPrediction-master\n",
      "--2018-03-29 15:39:23--  http://en.wiktionary.org/wiki/awesome\n",
      "Resolving en.wiktionary.org (en.wiktionary.org)... 198.35.26.96, 2620:0:863:ed1a::1\n",
      "Connecting to en.wiktionary.org (en.wiktionary.org)|198.35.26.96|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 TLS Redirect\n",
      "Location: https://en.wiktionary.org/wiki/awesome [following]\n",
      "--2018-03-29 15:39:24--  https://en.wiktionary.org/wiki/awesome\n",
      "Connecting to en.wiktionary.org (en.wiktionary.org)|198.35.26.96|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘test_s3/awesome’\n",
      "\n",
      "awesome                 [  <=>               ]  76.21K   196KB/s    in 0.4s    \n",
      "\n",
      "2018-03-29 15:39:25 (196 KB/s) - ‘test_s3/awesome’ saved [78038]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get a wikipedia page and store it in a local folder\n",
    "!pwd\n",
    "!mkdir test_s3\n",
    "!wget http://en.wiktionary.org/wiki/awesome -P test_s3/ --trust-server-names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a S3 bucket : provide a unique name below: replace the \"testaaset1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_bucket failed: s3://jeonghyonkim-seoul-files/NYTaxi/ An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it.\n",
      "upload: test_s3/awesome to s3://jeonghyonkim-seoul-files/NYTaxi/awesome\n"
     ]
    }
   ],
   "source": [
    "# Add the downloaded file to S3: remeber to replace \"testaaset1\" with a unique bucket name\n",
    "!aws s3 mb s3://jeonghyonkim-seoul-files/NYTaxi/\n",
    "    \n",
    "# Add the downloaded file to the test bucket in S3: remeber to replace \"testaaset1\" with a unique bucket name\n",
    "!aws s3 cp test_s3/awesome s3://jeonghyonkim-seoul-files/NYTaxi/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test if you are able to lookup the S3 file from Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/parallels/Documents/TaxiPrediction-master\n",
      "awesome\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls /home/parallels/Documents/TaxiPrediction-master/test_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "439"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testS3RDD = sc.textFile(\"/home/parallels/Documents/TaxiPrediction-master/test_s3/awesome\")\n",
    "testS3RDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Congrats! Now you have a working spark cluster with ability to connect with S3!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/parallels/Documents/TaxiPrediction-master\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the data files into local folder\n",
    "\n",
    "**Note**: We are downloading the data from **2013 onwards only** - though data is available from 2009\n",
    "\n",
    "**Data source**: http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup the variables\n",
    "\n",
    "baseUrl = \"/home/parallels/Documents/TaxiPrediction-master/data/nyc/\"\n",
    "#Yellow/green cab filename prefix\n",
    "yCabFNPrefix = \"/yellow_tripdata_\"\n",
    "gCabFNPrefix = \"/green_tripdata_\"\n",
    "\n",
    "#Availaiblity of data set by month & year\n",
    "yDict = {}\n",
    "gDict = {}\n",
    "\n",
    "#availablity for Yellow cab\n",
    "yDict[2015] = range(1,1) #available till jun 2015\n",
    "yDict[2014] = range(1,1)\n",
    "yDict[2013] = range(1,1)\n",
    "\n",
    "#availablity for Green cab\n",
    "gDict[2015] = range(1,1) #available till jun 2015\n",
    "gDict[2014] = range(1,1)\n",
    "gDict[2013] = range(8,13) #avialable only from august 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Yellow cab data file name list\n",
    "# file name is of format:  yellow_tripdata_2015-01.csv\n",
    "yCabUrls = []\n",
    "yCabFilenames = []\n",
    "for year, monthList in yDict.iteritems():\n",
    "    yearStr = str(year)\n",
    "    for month in monthList:\n",
    "        monthStr = str(month)\n",
    "        if len(monthStr) == 1:\n",
    "            monthStr = \"0\"+monthStr    \n",
    "        url = baseUrl+yearStr+yCabFNPrefix+yearStr+'-'+monthStr+\".csv\"\n",
    "        yCabUrls.append(url)\n",
    "        yCabFilenames.append(yCabFNPrefix+yearStr+'-'+monthStr+\".csv\")\n",
    "\n",
    "#  green cab data file name list\n",
    "gCabUrls = []\n",
    "gCabFilenames = []\n",
    "for year, monthList in gDict.iteritems():\n",
    "    yearStr = str(year)\n",
    "    for month in monthList:\n",
    "        monthStr = str(month)\n",
    "        if len(monthStr) == 1:\n",
    "            monthStr = \"0\"+monthStr    \n",
    "        url = baseUrl+yearStr+gCabFNPrefix+yearStr+'-'+monthStr+\".csv\"\n",
    "        gCabFilenames.append(gCabFNPrefix+yearStr+'-'+monthStr+\".csv\")\n",
    "        gCabUrls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180M\tdata/nyc\r\n"
     ]
    }
   ],
   "source": [
    "#Disk space of the Yellow Cab files\n",
    "!du -mh data/nyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/green_tripdata_2013-08.csv', '/green_tripdata_2013-09.csv', '/green_tripdata_2013-10.csv', '/green_tripdata_2013-11.csv', '/green_tripdata_2013-12.csv']\n"
     ]
    }
   ],
   "source": [
    "print gCabFilenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(cabFilenames, isYellow):\n",
    "    \"\"\"\n",
    "    Function that takes a list of filenames (strings) and a boolean as parameters.\n",
    "    Removes the header from the each file and verifies the schema of the data.\n",
    "    \"\"\"\n",
    "    # Dictionary where key = filename, value = (schema, bool==True if there is a blank line after header)\n",
    "    file_schemas = {}\n",
    "    prefix = 'data/nyc'\n",
    "    if isYellow:\n",
    "        prefix = 'data/nyc'\n",
    "        \n",
    "    for filename in cabFilenames:\n",
    "        # Fetch schema\n",
    "        with open(prefix+filename,'r') as in_fp:\n",
    "            #read first two lines\n",
    "            lines = [in_fp.readline() for i in xrange(2)]\n",
    "\n",
    "        # now open again to write out\n",
    "        file_schemas[filename] = (tuple(lines[0].split(',')), lines[1]=='\\r\\n')\n",
    "    \n",
    "    # verify all files have the necessary columns in the same position\n",
    "    for (schema,blank) in file_schemas.values():\n",
    "        assert 'ickup' in schema[1]\n",
    "        assert 'atetime' in schema[1]\n",
    "        assert 'ickup' in schema[5]\n",
    "        assert 'ongitude' in schema[5]\n",
    "        assert 'ickup' in schema[6]\n",
    "        assert 'atitude' in schema[6]\n",
    "    print \"Schema:\", file_schemas[filename][0]\n",
    "    \n",
    "    # Remove header and blank line from file\n",
    "    for filename in cabFilenames:\n",
    "        print \"Writing to %r\" % filename \n",
    "        with open(prefix+filename,'r') as in_fp:\n",
    "            #read whole file\n",
    "            lines = in_fp.readlines()\n",
    "\n",
    "        with open(prefix+filename,'w') as out_fp:\n",
    "\n",
    "            # check if there is a blank line after the header\n",
    "            if file_schemas[filename][1]:\n",
    "                out_fp.writelines(lines[2:])\n",
    "            else:\n",
    "                out_fp.writelines(lines[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema: ('VendorID', 'lpep_pickup_datetime', 'Lpep_dropoff_datetime', 'Store_and_fwd_flag', 'RateCodeID', 'Pickup_longitude', 'Pickup_latitude', 'Dropoff_longitude', 'Dropoff_latitude', 'Passenger_count', 'Trip_distance', 'Fare_amount', 'Extra', 'MTA_tax', 'Tip_amount', 'Tolls_amount', 'Ehail_fee', 'Total_amount', 'Payment_type', 'Trip_type \\n')\n",
      "Writing to '/green_tripdata_2013-08.csv'\n",
      "Writing to '/green_tripdata_2013-09.csv'\n",
      "Writing to '/green_tripdata_2013-10.csv'\n",
      "Writing to '/green_tripdata_2013-11.csv'\n",
      "Writing to '/green_tripdata_2013-12.csv'\n"
     ]
    }
   ],
   "source": [
    "#Preprocess Yellow Cab files -- check schema\n",
    "preprocess_data(gCabFilenames, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Check cluster performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 2.87 ms\n",
      "CPU times: user 0 ns, sys: 12 ms, total: 12 ms\n",
      "Wall time: 14.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12450521"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD = sc.textFile(\"s3://sdaultonbucket1/nyc/yellow_tripdata_2015-02.csv\")\n",
    "%time myRDD.cache()\n",
    "%time myRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.is_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other useful notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Enable the web admin interface** from the AWS console (follow the steps it says). Note: in this step when you open the SSH conection (as per instructions), the connection might not show ANY thing status etc) - this is fine. The SSH command (As per instruction) is:  ssh -i CS109.pem $DNS_NAME -ND 8157\n",
    "\n",
    "**2. Admin UI's**\n",
    "    \n",
    "    a) To get to the Spark Jobs Admin Console: Go to the Hadoop Resource Manager UI (from AWS console) and click on \"Application master\" link (it will be one of the items in the listed running applications).\n",
    "    \n",
    "    b) Spark history server: http://<domain>:18080/\n",
    "    \n",
    "    c) For CPU/Memory performance on each node use the Ganglia UI (link from the AWS console)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
